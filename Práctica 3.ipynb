{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 3\n",
    "\n",
    "**IDS330L, Feb-Abr 2023**\n",
    "\n",
    "1. Jugar con la estructura de la red (página 47 del libro)\n",
    "2. Cambiar a Tensorflow (paginas 50-55 del libro)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    \"\"\"A simple fully-connected NN layer.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        layer_size (int): The output vector size / number of neurons in the layer.\n",
    "        activation_function (callable): The activation function for this layer.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (ndarray): The bias value, added to the weighted sum.\n",
    "        size (int): The layer size / number of neurons.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "        x (ndarray): The last provided input vector, stored for backpropagation.\n",
    "        y (ndarray): The corresponding output, also stored for backpropagation.\n",
    "        derivated_activation_function (callable): The corresponding derivated function for backpropagation.\n",
    "        dL_dW (ndarray): The derivative of the loss, with respect to the weights W.\n",
    "        dL_db (ndarray): The derivative of the loss, with respect to the bias b.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, layer_size, activation_function, derivated_activation_function=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Randomly initializing the weight vector and the bias value (using a normal distribution this time):\n",
    "        self.W = np.random.standard_normal((num_inputs, layer_size))\n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.derivated_activation_function = derivated_activation_function\n",
    "        self.x, self.y = None, None\n",
    "        self.dL_dW, self.dL_db = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layer, returning its activation vector.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`\n",
    "        Returns:\n",
    "            activation (ndarray): The activation value, of shape `(batch_size, layer_size)`.\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        self.y = self.activation_function(z)\n",
    "        self.x = x  # (we store the input and output values for back-propagation)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Back-propagate the loss, computing all the derivatives, storing those w.r.t. the layer parameters,\n",
    "        and returning the loss w.r.t. its inputs for further propagation.\n",
    "        Args:\n",
    "            dL_dy (ndarray): The loss derivative w.r.t. the layer's output (dL/dy = l'_{k+1}).\n",
    "        Returns:\n",
    "            dL_dx (ndarray): The loss derivative w.r.t. the layer's input (dL/dx).\n",
    "        \"\"\"\n",
    "        dy_dz = self.derivated_activation_function(self.y)  # = f'\n",
    "        dL_dz = (dL_dy * dy_dz) # dL/dz = dL/dy * dy/dz = l'_{k+1} * f'\n",
    "        dz_dw = self.x.T\n",
    "        dz_dx = self.W.T\n",
    "        dz_db = np.ones(dL_dy.shape[0]) # dz/db = d(W.x + b)/db = 0 + db/db = \"ones\"-vector\n",
    "\n",
    "        # Computing the derivatives with respect to the layer's parameters, and storing them for opt. optimization:\n",
    "        self.dL_dW = np.dot(dz_dw, dL_dz)\n",
    "        self.dL_db = np.dot(dz_db, dL_dz)\n",
    "\n",
    "        # Computing the derivative with respect to the input, to be passed to the previous layers (their `dL_dy`):\n",
    "        dL_dx = np.dot(dL_dz, dz_dx)\n",
    "        return dL_dx\n",
    "\n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"\n",
    "        Optimize the layer's parameters, using the stored derivative values.\n",
    "        Args:\n",
    "            epsilon (float): The learning rate.\n",
    "        \"\"\"\n",
    "        self.W -= epsilon * self.dL_dW\n",
    "        self.b -= epsilon * self.dL_db  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):             # sigmoid function\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def derivated_sigmoid(y):   # sigmoid derivative function\n",
    "    return y * (1 - y)\n",
    "\n",
    "def loss_L2(pred, target):             # L2 loss function\n",
    "    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. we divide by the batch size\n",
    "\n",
    "\n",
    "def derivated_loss_L2(pred, target):   # L2 derivative function\n",
    "    return 2 * (pred - target)\n",
    "\n",
    "def binary_cross_entropy(pred, target):            # cross-entropy loss function\n",
    "    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n",
    "\n",
    "\n",
    "def derivated_binary_cross_entropy(pred, target):  # cross-entropy derivative function\n",
    "    return (pred - target) / (pred * (1 - pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(object):\n",
    "    \"\"\"A simple fully-connected NN.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        num_outputs (int): The output vector size.\n",
    "        hidden_layers_sizes (list): A list of sizes for each hidden layer to add to the network\n",
    "        activation_function (callable): The activation function for all the layers\n",
    "        derivated_activation_function (callable): The derivated activation function\n",
    "        loss_function (callable): The loss function to train this network\n",
    "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation\n",
    "    Attributes:\n",
    "        layers (list): The list of layers forming this simple network.\n",
    "        loss_function (callable): The loss function to train this network.\n",
    "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n",
    "                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n",
    "                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n",
    "        super().__init__()\n",
    "        # We build the list of layers composing the network, according to the provided arguments:\n",
    "        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n",
    "        self.layers = [\n",
    "            FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1], \n",
    "                                activation_function, derivated_activation_function)\n",
    "            for i in range(len(layer_sizes) - 1)]\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.derivated_loss_function = derivated_loss_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layers, returning the output vector.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`.\n",
    "        Returns:\n",
    "            activation (ndarray): The output activation value, of shape `(batch_size, layer_size)`.\n",
    "        \"\"\"\n",
    "        for layer in self.layers: # from the input layer to the output one\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Compute the output corresponding to input `x`, and return the index of the largest \n",
    "        output value.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(1, num_inputs)`.\n",
    "        Returns:\n",
    "            best_class (int): The predicted class ID.\n",
    "        \"\"\"\n",
    "        estimations = self.forward(x)\n",
    "        best_class = np.argmax(estimations)\n",
    "        return best_class\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Back-propagate the loss hrough the layers (require `forward()` to be called before).\n",
    "        Args:\n",
    "            dL_dy (ndarray): The loss derivative w.r.t. the network's output (dL/dy).\n",
    "        Returns:\n",
    "            dL_dx (ndarray): The loss derivative w.r.t. the network's input (dL/dx).\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers): # from the output layer to the input one\n",
    "            dL_dy = layer.backward(dL_dy)\n",
    "        return dL_dy\n",
    "\n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"\n",
    "        Optimize the network parameters according to the stored gradients (require `backward()`\n",
    "        to be called before).\n",
    "        Args:\n",
    "            epsilon (float): The learning rate.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:             # the order doesn't matter here\n",
    "            layer.optimize(epsilon)\n",
    "\n",
    "    def evaluate_accuracy(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
    "        Args:\n",
    "            X_val (ndarray): The input validation dataset.\n",
    "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
    "        Returns:\n",
    "            accuracy (float): The accuracy of the network \n",
    "                              (= number of correct predictions/dataset size).\n",
    "        \"\"\"\n",
    "        num_corrects = 0\n",
    "        for i in range(len(X_val)):\n",
    "            pred_class = self.predict(X_val[i])\n",
    "            if pred_class == y_val[i]:\n",
    "                num_corrects += 1\n",
    "        return num_corrects / len(X_val)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=32, num_epochs=5, learning_rate=1e-3, print_frequency=20):\n",
    "        \"\"\"\n",
    "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
    "        Args:\n",
    "            X_train (ndarray): The input training dataset.\n",
    "            y_train (ndarray): The corresponding ground-truth training dataset.\n",
    "            X_val (ndarray): The input validation dataset.\n",
    "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
    "            batch_size (int): The mini-batch size.\n",
    "            num_epochs (int): The number of training epochs i.e. iterations over the whole dataset.\n",
    "            learning_rate (float): The learning rate to scale the derivatives.\n",
    "            print_frequency (int): Frequency to print metrics (in epochs).\n",
    "        Returns:\n",
    "            losses (list): The list of training losses for each epoch.\n",
    "            accuracies (list): The list of validation accuracy values for each epoch.\n",
    "        \"\"\"\n",
    "        num_batches_per_epoch = len(X_train) // batch_size\n",
    "        do_validation = X_val is not None and y_val is not None\n",
    "        losses, accuracies = [], []\n",
    "        for i in range(num_epochs): # for each training epoch\n",
    "            epoch_loss = 0\n",
    "            for b in range(num_batches_per_epoch):  # for each batch composing the dataset\n",
    "                # Get batch:\n",
    "                batch_index_begin = b * batch_size\n",
    "                batch_index_end = batch_index_begin + batch_size\n",
    "                x = X_train[batch_index_begin: batch_index_end]\n",
    "                targets = y_train[batch_index_begin: batch_index_end]\n",
    "                # Optimize on batch:\n",
    "                predictions = y = self.forward(x)  # forward pass\n",
    "                L = self.loss_function(predictions, targets)  # loss computation\n",
    "                dL_dy = self.derivated_loss_function(predictions, targets)  # loss derivation\n",
    "                self.backward(dL_dy)  # back-propagation pass\n",
    "                self.optimize(learning_rate)  # optimization of the NN\n",
    "                epoch_loss += L\n",
    "\n",
    "            # Logging training loss and validation accuracy, to follow the training:\n",
    "            epoch_loss /= num_batches_per_epoch\n",
    "            losses.append(epoch_loss)\n",
    "            if do_validation:\n",
    "                accuracy = self.evaluate_accuracy(X_val, y_val)\n",
    "                accuracies.append(accuracy)\n",
    "            else:\n",
    "                accuracy = np.NaN\n",
    "            if i % print_frequency == 0 or i == (num_epochs - 1):\n",
    "                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "                    i, epoch_loss, accuracy * 100))\n",
    "        return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install matplotlib  # Uncomment and run if matplotlib is not installed yet.\n",
    "import matplotlib          # We use this package to visualize some data and results\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Preparamos los datos de las imágenes\n",
    "X_train, y_train = mnist.train_images(), mnist.train_labels()\n",
    "X_test,  y_test  = mnist.test_images(), mnist.test_labels()\n",
    "num_classes = 10    # classes are the digits from 0 to 9\n",
    "\n",
    "# Cambia las imágenes de matrices 28x28 a vectores 1x784\n",
    "X_train, X_test = X_train.reshape(-1, 28 * 28), X_test.reshape(-1, 28 * 28)\n",
    "print(\"Pixel values between {} and {}\".format(X_train.min(), X_train.max()))\n",
    "\n",
    "# Normaliza los vectores\n",
    "X_train, X_test = X_train / 255., X_test / 255.\n",
    "print(\"Normalized pixel values between {} and {}\".format(X_train.min(), X_train.max()))\n",
    "y_train = np.eye(num_classes)[y_train] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cambiar la estructura de la red\n",
    "\n",
    "Para cambiar la manera en la que la red predice la información, podemos variar la su composición interna, como la la cantidad de capas  ocultas o la función de activación.\n",
    "\n",
    "En este caso, manipularemos la cantidad o el tamaño de las capas, para determinar si esto tiene algún resultado distinto.\n",
    "\n",
    "Hay que tener cuidado con los tamaños de las redes, pues se cambian la cantidad de parámetros que hay que manipular (los coeficientes para las sumas y multiplicaciones).\n",
    "\n",
    "Podemos crear una red con una estructura específica con algo como\n",
    "\n",
    "```python\n",
    "clasificador_numeros = SimpleNetwork(num_inputs=X_train.shape[1], num_outputs=num_classes, hidden_layers_sizes=[64, 32])\n",
    "```\n",
    "\n",
    "donde las capas internas son de 64 y 32 neuronas respectivamente.\n",
    "\n",
    "Entrenamos el clasificador con algo como:\n",
    "\n",
    "```python\n",
    "print(\"Entrenando a 80 epochs:\")\n",
    "losses, accuracies = clasificador_numeros.train(X_train, y_train, X_test, y_test, batch_size=30, num_epochs=80)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un dato importante a considerar es que nuestra clase `FullyConnectedLayer` para el clasificador está siendo creada con una capa de entrada con 784 neuronas (verifique probando el valor que devuelve `X_train.shape[1]`, que se usa al crear la red).\n",
    "\n",
    "Esto implica que la ecuación (pg. 33)\n",
    "\n",
    "$$ z = x \\cdot W + B $$\n",
    "\n",
    "donde $W$ y $B$ combinan los vectores de ganancias y bias de cada neurona, tiene una cantidad considerable de coeficientes (ganancias y bias) que calcular. Por tanto, hay que elegir apropiadamente el número y tamaño de capas ocultas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "\n",
    "1. Analice y escriba cuántos coeficientes (ganancias y bias) se necesitan para una red que acepte como entrada 784 valores, tenga dos capas ocultas de 64 y 32 neuronas, y 10 neuronas en la capa de salida. Justifique.\n",
    "2. Seleccione de 8 imágenes elegidas aleatoriamente de las imágenes de prueba `X_test`.\n",
    "3. Cree una red con dos capas internas de 64 neuronas, entrenela a por lo menos 100 `epochs` y pruébela con las 8 imágenes\n",
    "4. Cree una red con dos capas internas de 32 y 64,  entrene a por lo menos 100 `epochs` y pruébela con las 8 imágenes\n",
    "5. Cree una red con una capa interna de 128 neuronas,  entrene a por lo menos 100 `epochs` y pruébela con las 8 imágenes.\n",
    "6. Compare los resultados, ¿qué diferencia cree que haya entre cada una?\n",
    "6. ¿Cuántos coeficientes hubo en cada una?\n",
    "8. Corra el código debajo, basado en _Tensorflow_ (**explicado en pgs. 50-55** del libro)y compare la precisión de validación (`val_accuracy`) y el número de _epochs_ con la red del punto 5. Ambas tienen la misma estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_muestra_prueba = np.random.randint(0, X_test.shape[0], 8)\n",
    "print(\"Índices de las imágenes seleccionadas: {}\".format(indices_muestra_prueba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_64x64 = SimpleNetwork(num_inputs=X_train.shape[1], \n",
    "                                 num_outputs=num_classes, hidden_layers_sizes=[64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entrenando a 100 epochs:\")\n",
    "losses, accuracies = red_64x64.train(X_train, y_train, X_test, y_test, \n",
    "                                            batch_size=30, num_epochs=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pruebando red de 64+64 :\")\n",
    "# Predecir en número en cada una de las 8 imágenes\n",
    "for img in indices_muestra_prueba:\n",
    "    predicted_class = red_64x64.predict(np.expand_dims(X_test[img], 0))\n",
    "    print('Predicted class: {}; Correct class: {}'.format(predicted_class, y_test[img]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trabajar con Tensorflow\n",
    "\n",
    "\n",
    "Usaremos _Tensorflow_ para la misma tarea, siguiendo el ejemplo del libro. En este caso, la librería incluye su versión de las imágenes del set de datos MNIST con el que hemos trabajado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparar las imágenes. Necesita conexión a internet para la primera vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "num_channels = 1\n",
    "input_shape = (img_rows, img_cols, num_channels)\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo simple de una red clasificadora\n",
    "\n",
    "Este modelo se explica en la página 53 del libro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento de una red en TensorFlow\n",
    "\n",
    "Explicado en la página 54.\n",
    "\n",
    "El método `compile()` debe ser llamado antes del entrenamiento (método `fit()`).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "callbacks = [tf.keras.callbacks.TensorBoard('./keras')]\n",
    "model.fit(x_train, y_train, epochs=25, verbose=1, validation_data=(x_test, y_test), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ids-ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f601792c76910078458e470cd9c93a0ebec751bec328425a1820cc8688c8ca05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
