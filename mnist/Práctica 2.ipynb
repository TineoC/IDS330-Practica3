{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Práctica 2\n",
    "**IDS330L, Feb-Abr 2023**\n",
    "\n",
    "En continuación a la introducción a uno de los conceptos básicos de las implementación de inteligencia en las máquinas, veremos la aplicación más ventajosa de una neurona: combinar informaciones para tomar una decisión.\n",
    "\n",
    "En este caso veremos: \n",
    "1. La organización de neuronas para tomar una decisión\n",
    "2. El proceso de ajustar con una meta los pesos de la suma ponderada que lleva a cabo cada neurona\n",
    "3. La implementación de un método de propagación de los ajustes para cambiar una serie de neuronas hacia un resultado esperado\n",
    "\n",
    "Esto es, veremos cómo crear y entrenar una red neuronal para hacer un trabajo de clasificación, y las herramientas y procesos que lo permiten.\n",
    "\n",
    "Está práctica está basada en las explicaciones del libro de (Planche & Andres, 2019) entre las páginas 31-47 y hará constantes referencias a este texto. \n",
    "\n",
    "Esta sección sigue los pasos mostrados en el notebook de acompañamiento al capítulo 1 del libro, que está publicado en https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ventaja de las neuronas es que convierten una serie de entradas en una sola salida que responde de acuerdo al estímulo combinado de sus entradas. Es decir, una neurona permite combinar 2 o más entradas para generar una salida, que hemos expresado como \n",
    "\n",
    "$$ z = x \\cdot w + b $$\n",
    "\n",
    "que es la implementación de un producto punto entre las entradas y los pesos (ganancias). \n",
    "\n",
    "Como explica el libro en la página 32, esta implementacion se puede expandir si se consideran varias neuronas que procesan cada una (en paralelo) las mismas $n$ entradas, pero con ganancias úncas para cada neurona. Esto es que \n",
    "\n",
    "$$ z_A = x \\cdot w_A + b_A \\\\  z_B = x \\cdot w_B + b_B  \\\\  z_C = x \\cdot w_C + b_C $$\n",
    "\n",
    "se puede resumir a \n",
    "\n",
    "$$ z = x \\cdot W + B $$\n",
    "\n",
    "donde $W$ y $B$ combinan los vectores de ganancias y bias de cada neurona. Note que la operación sigue siendo un producto punto. \n",
    "\n",
    "Esto implica que para un vector $x$ de entradas, se pueden crear $W$ y $B$ de forma que en una sola operación se computen las salidas $z$ de cada $n$ neuronas (ver página 33).\n",
    "\n",
    "Más interesantemente, esto también puede expandirse para computar las salidas $z$ de **varios** vectores de entrada al mismo tiempo. Es decir, para computar las salidas de $n$ neuronas, se puede pasar un vector $x$ de tamaño $n x s$ ($s$ entradas) para generar un vector de salidas $z$ de tamaño $n x 1$ ($n$ salidas), sin tener que cambiar la manera en que se computan.\n",
    "\n",
    "Esto quiere decir que con definir la versión matricial descrita arriba, se pueden implementar varias neuronas trabajando juntas para procesar una misma serie de entradas $x$ para generar una serie de salidas $z$, esencialmente implementando **una capa de neuronas** interconectadas entre ellas (todas interpretando la misma entrada y generando su propia salida). Esta es la base para las redes neuronales, que implementan varias capas de neuronas en serie que finalmente generan una salida que resume la interpretación de la entrada.\n",
    "\n",
    "La clase mostrada abajo implementa una capa de neuronas interconectadas basada en la ecuación matricial de arriba. Si notan la diferencia esencial entre esta y la de la neurona simple es la forma en que se acepta un tamaño de la red (cantidad de neuronas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    \"\"\"A simple fully-connected NN layer.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        layer_size (int): The output vector size / number of neurons in the layer.\n",
    "        activation_function (callable): The activation function for this layer.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (ndarray): The bias value, added to the weighted sum.\n",
    "        size (int): The layer size / number of neurons.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "        x (ndarray): The last provided input vector, stored for backpropagation.\n",
    "        y (ndarray): The corresponding output, also stored for backpropagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, layer_size, activation_function, derivated_activation_function=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Randomly initializing the weight vector and the bias value (using a normal distribution this time):\n",
    "        self.W = np.random.standard_normal((num_inputs, layer_size))\n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.derivated_activation_function = derivated_activation_function\n",
    "        self.x, self.y = None, None\n",
    "        self.dL_dW, self.dL_db = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layer, returning its activation vector.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`\n",
    "        Returns:\n",
    "            activation (ndarray): The activation value, of shape `(batch_size, layer_size)`.\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        self.y = self.activation_function(z)\n",
    "        self.x = x  # (we store the input and output values for back-propagation)\n",
    "        return self.y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos la habilidad de combinar entradas en una red de 3 neuronas donde cada una procesa las mismas 2 entradas. Luego alimentaremos dos grupos de entradas diferentes en una sola operación.\n",
    "\n",
    "Primero generamos la red Y generamos las entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size    = 2\n",
    "num_neurons   = 3\n",
    "relu_function = lambda y: np.maximum(y, 0)\n",
    "\n",
    "layer = FullyConnectedLayer(num_inputs=input_size, layer_size=num_neurons, activation_function=relu_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n",
    "print(\"Input vector #1: {}\".format(x1))\n",
    "x2 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n",
    "print(\"Input vector #2: {}\".format(x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos la red con ada entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = layer.forward(x1)\n",
    "print(\"Layer's output value given `x1` : {}\".format(y1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = layer.forward(x2)\n",
    "print(\"Layer's output value given `x2` : {}\".format(y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y combinamos ambas entradas en una sola operación, para comparar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x12 = np.concatenate((x1, x2))  # stack of input vectors, of shape `(2, 2)`\n",
    "y12 = layer.forward(x12)\n",
    "print(\"Layer's output value given `[x1, x2]` :\\n{}\".format(y12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creando una red de neuronas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta vez, prepararemos una red capaz de reconocer los número 0-9 a partir de imágenes. Esto es, una red capaz de clasificar imágenes de acuerdo si la red interpreta que la imagen tiene un número entre el 0 y el 9. La forma en que esto ocurre es creando una red que acepte la imagen (sus pixeles) en su capa de entrada, y active solo una neurona en la capa de salida, que represente el valor del número. Esto es, que una neurona se activa si es un 1, otra se activa si es un 2, y así.\n",
    "\n",
    "Combinar varias capas no es suficiente para hacer un trabajo, cada neurona necesita responder adecuadamente a los estímulos que recibe para que el resultado final sea el esperado. Sin embargo, predefinir el valor específico de cada ganancia puede llegar a ser imposible. Si no es así, lo que tenemos es una serie de capas de neuronas respondiendo aleatoriamente a los estímulo, ya que cada neurona se inicializa con valores aleatorios.\n",
    "\n",
    "Sin embargo, podemos generar una red que haga un trabajo específico si conseguimos una manera de ir ajustando las ganancias apropiadamente según los resultados que veamos. Esto es entrenar una red neuronal y lo veremos abajo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basado en el cuaderno disponible por el libro.\n",
    "\n",
    "Nuestro propósito es combinar varias capas (que pueden tener diferentes tamaños) para crear una **red neuronal** que nos permitirá realizar predicciones no lineales.\n",
    "\n",
    "Aplicando el método de ***descenso de gradiente***, dicha red se puede entrenar para realizar predicciones correctas (ver la teoría en las páginas 38-42 del libro). Pero para eso, necesitamos definir una función de ***pérdida*** para evaluar el rendimiento de la red (cf. ***L2*** o pérdidas de ***entropía cruzada*** presentadas en el libro), y necesita saber cómo *derivar* todas las operaciones realizadas por la red, para calcular y propagar los gradientes entre las neuronas de cada red.\n",
    "\n",
    "En esta sección, presentaremos cómo se puede construir una red neuronal simple completamente conectada. Supongamos que queremos que nuestra red use la función *sigmoide* para la activación. Necesitamos implementar esa función _y_ su derivada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):             # sigmoid function\n",
    "    y = 1 / (1 + np.exp(-x))\n",
    "    return y\n",
    "\n",
    "\n",
    "def derivated_sigmoid(y):   # sigmoid derivative function\n",
    "    return y * (1 - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En una red para clasificación, pudieramos usar una función de pérdida tipo *L2* o de *entropía cruzada*, como explica el libro (pg. 40). Y se debe implementar la función *L2*, la de entropía y sus derivadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_L2(pred, target):             # L2 loss function\n",
    "    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. we divide by the batch size\n",
    "\n",
    "\n",
    "def derivated_loss_L2(pred, target):   # L2 derivative function\n",
    "    return 2 * (pred - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(pred, target):            # cross-entropy loss function\n",
    "    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n",
    "\n",
    "\n",
    "def derivated_binary_cross_entropy(pred, target):  # cross-entropy derivative function\n",
    "    return (pred - target) / (pred * (1 - pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para entrenar la red, necesitamos darle la opción de poder actualizarse utilizando funciones de optimización e implementando un método de propagación reversa de los ajustes. La clase debajo lo actualiza la clase de Capa para tener esta habilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    \"\"\"A simple fully-connected NN layer.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        layer_size (int): The output vector size / number of neurons in the layer.\n",
    "        activation_function (callable): The activation function for this layer.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (ndarray): The bias value, added to the weighted sum.\n",
    "        size (int): The layer size / number of neurons.\n",
    "        activation_function (callable): The activation function computing the neuron's output.\n",
    "        x (ndarray): The last provided input vector, stored for backpropagation.\n",
    "        y (ndarray): The corresponding output, also stored for backpropagation.\n",
    "        derivated_activation_function (callable): The corresponding derivated function for backpropagation.\n",
    "        dL_dW (ndarray): The derivative of the loss, with respect to the weights W.\n",
    "        dL_db (ndarray): The derivative of the loss, with respect to the bias b.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, layer_size, activation_function, derivated_activation_function=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # Randomly initializing the weight vector and the bias value (using a normal distribution this time):\n",
    "        self.W = np.random.standard_normal((num_inputs, layer_size))\n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "\n",
    "        self.activation_function = activation_function\n",
    "        self.derivated_activation_function = derivated_activation_function\n",
    "        self.x, self.y = None, None\n",
    "        self.dL_dW, self.dL_db = None, None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layer, returning its activation vector.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`\n",
    "        Returns:\n",
    "            activation (ndarray): The activation value, of shape `(batch_size, layer_size)`.\n",
    "        \"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        self.y = self.activation_function(z)\n",
    "        self.x = x  # (we store the input and output values for back-propagation)\n",
    "        return self.y\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Back-propagate the loss, computing all the derivatives, storing those w.r.t. the layer parameters,\n",
    "        and returning the loss w.r.t. its inputs for further propagation.\n",
    "        Args:\n",
    "            dL_dy (ndarray): The loss derivative w.r.t. the layer's output (dL/dy = l'_{k+1}).\n",
    "        Returns:\n",
    "            dL_dx (ndarray): The loss derivative w.r.t. the layer's input (dL/dx).\n",
    "        \"\"\"\n",
    "        dy_dz = self.derivated_activation_function(self.y)  # = f'\n",
    "        dL_dz = (dL_dy * dy_dz) # dL/dz = dL/dy * dy/dz = l'_{k+1} * f'\n",
    "        dz_dw = self.x.T\n",
    "        dz_dx = self.W.T\n",
    "        dz_db = np.ones(dL_dy.shape[0]) # dz/db = d(W.x + b)/db = 0 + db/db = \"ones\"-vector\n",
    "\n",
    "        # Computing the derivatives with respect to the layer's parameters, and storing them for opt. optimization:\n",
    "        self.dL_dW = np.dot(dz_dw, dL_dz)\n",
    "        self.dL_db = np.dot(dz_db, dL_dz)\n",
    "\n",
    "        # Computing the derivative with respect to the input, to be passed to the previous layers (their `dL_dy`):\n",
    "        dL_dx = np.dot(dL_dz, dz_dx)\n",
    "        return dL_dx\n",
    "\n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"\n",
    "        Optimize the layer's parameters, using the stored derivative values.\n",
    "        Args:\n",
    "            epsilon (float): The learning rate.\n",
    "        \"\"\"\n",
    "        self.W -= epsilon * self.dL_dW\n",
    "        self.b -= epsilon * self.dL_db  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementación de la red"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para crear la red, usamos una clase que implemente diferente capas de neuronas y las funciones para pasar la información por cada red.\n",
    "\n",
    "Esta red necesitará poder realizar los cálculos necesarios para definir los ajustes de las ganancias en cada neurona, y propagarlas hasta alcanzar cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(object):\n",
    "    \"\"\"A simple fully-connected NN.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        num_outputs (int): The output vector size.\n",
    "        hidden_layers_sizes (list): A list of sizes for each hidden layer to add to the network\n",
    "        activation_function (callable): The activation function for all the layers\n",
    "        derivated_activation_function (callable): The derivated activation function\n",
    "        loss_function (callable): The loss function to train this network\n",
    "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation\n",
    "    Attributes:\n",
    "        layers (list): The list of layers forming this simple network.\n",
    "        loss_function (callable): The loss function to train this network.\n",
    "        derivated_loss_function (callable): The derivative of the loss function, for back-propagation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n",
    "                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n",
    "                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n",
    "        super().__init__()\n",
    "        # We build the list of layers composing the network, according to the provided arguments:\n",
    "        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n",
    "        self.layers = [\n",
    "            FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1], \n",
    "                                activation_function, derivated_activation_function)\n",
    "            for i in range(len(layer_sizes) - 1)]\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.derivated_loss_function = derivated_loss_function\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward the input vector through the layers, returning the output vector.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(batch_size, num_inputs)`.\n",
    "        Returns:\n",
    "            activation (ndarray): The output activation value, of shape `(batch_size, layer_size)`.\n",
    "        \"\"\"\n",
    "        for layer in self.layers: # from the input layer to the output one\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Compute the output corresponding to input `x`, and return the index of the largest \n",
    "        output value.\n",
    "        Args:\n",
    "            x (ndarray): The input vector, of shape `(1, num_inputs)`.\n",
    "        Returns:\n",
    "            best_class (int): The predicted class ID.\n",
    "        \"\"\"\n",
    "        estimations = self.forward(x)\n",
    "        best_class = np.argmax(estimations)\n",
    "        return best_class\n",
    "\n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"\n",
    "        Back-propagate the loss hrough the layers (require `forward()` to be called before).\n",
    "        Args:\n",
    "            dL_dy (ndarray): The loss derivative w.r.t. the network's output (dL/dy).\n",
    "        Returns:\n",
    "            dL_dx (ndarray): The loss derivative w.r.t. the network's input (dL/dx).\n",
    "        \"\"\"\n",
    "        for layer in reversed(self.layers): # from the output layer to the input one\n",
    "            dL_dy = layer.backward(dL_dy)\n",
    "        return dL_dy\n",
    "\n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"\n",
    "        Optimize the network parameters according to the stored gradients (require `backward()`\n",
    "        to be called before).\n",
    "        Args:\n",
    "            epsilon (float): The learning rate.\n",
    "        \"\"\"\n",
    "        for layer in self.layers:             # the order doesn't matter here\n",
    "            layer.optimize(epsilon)\n",
    "\n",
    "    def evaluate_accuracy(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
    "        Args:\n",
    "            X_val (ndarray): The input validation dataset.\n",
    "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
    "        Returns:\n",
    "            accuracy (float): The accuracy of the network \n",
    "                              (= number of correct predictions/dataset size).\n",
    "        \"\"\"\n",
    "        num_corrects = 0\n",
    "        for i in range(len(X_val)):\n",
    "            pred_class = self.predict(X_val[i])\n",
    "            if pred_class == y_val[i]:\n",
    "                num_corrects += 1\n",
    "        return num_corrects / len(X_val)\n",
    "\n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=32, num_epochs=5, learning_rate=1e-3, print_frequency=20):\n",
    "        \"\"\"\n",
    "        Given a dataset and its ground-truth labels, evaluate the current accuracy of the network.\n",
    "        Args:\n",
    "            X_train (ndarray): The input training dataset.\n",
    "            y_train (ndarray): The corresponding ground-truth training dataset.\n",
    "            X_val (ndarray): The input validation dataset.\n",
    "            y_val (ndarray): The corresponding ground-truth validation dataset.\n",
    "            batch_size (int): The mini-batch size.\n",
    "            num_epochs (int): The number of training epochs i.e. iterations over the whole dataset.\n",
    "            learning_rate (float): The learning rate to scale the derivatives.\n",
    "            print_frequency (int): Frequency to print metrics (in epochs).\n",
    "        Returns:\n",
    "            losses (list): The list of training losses for each epoch.\n",
    "            accuracies (list): The list of validation accuracy values for each epoch.\n",
    "        \"\"\"\n",
    "        num_batches_per_epoch = len(X_train) // batch_size\n",
    "        do_validation = X_val is not None and y_val is not None\n",
    "        losses, accuracies = [], []\n",
    "        for i in range(num_epochs): # for each training epoch\n",
    "            epoch_loss = 0\n",
    "            for b in range(num_batches_per_epoch):  # for each batch composing the dataset\n",
    "                # Get batch:\n",
    "                batch_index_begin = b * batch_size\n",
    "                batch_index_end = batch_index_begin + batch_size\n",
    "                x = X_train[batch_index_begin: batch_index_end]\n",
    "                targets = y_train[batch_index_begin: batch_index_end]\n",
    "                # Optimize on batch:\n",
    "                predictions = y = self.forward(x)  # forward pass\n",
    "                L = self.loss_function(predictions, targets)  # loss computation\n",
    "                dL_dy = self.derivated_loss_function(predictions, targets)  # loss derivation\n",
    "                self.backward(dL_dy)  # back-propagation pass\n",
    "                self.optimize(learning_rate)  # optimization of the NN\n",
    "                epoch_loss += L\n",
    "\n",
    "            # Logging training loss and validation accuracy, to follow the training:\n",
    "            epoch_loss /= num_batches_per_epoch\n",
    "            losses.append(epoch_loss)\n",
    "            if do_validation:\n",
    "                accuracy = self.evaluate_accuracy(X_val, y_val)\n",
    "                accuracies.append(accuracy)\n",
    "            else:\n",
    "                accuracy = np.NaN\n",
    "            if i % print_frequency == 0 or i == (num_epochs - 1):\n",
    "                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "                    i, epoch_loss, accuracy * 100))\n",
    "        return losses, accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para nuestra tarea de clasificación, utilizaremos el [conjunto de datos MNIST](http://yann.lecun.com/exdb/mnist) presentado en el libro (Yann LeCun y Corinna Cortes tienen todos derechos de autor para este conjunto de datos).\n",
    "\n",
    "Antes de implementar una solución, debemos preparar los datos, cargando las imágenes MNIST para los métodos de entrenamiento y prueba. Para simplificar, usaremos el módulo Python [`mnist`](https://github.com/datapythonista/mnist) desarrollado por [Marc Garcia](https://github.com/datapythonista), y suplido junto a este cuaderno (ver [`./mnist/`](mnist/__init__.py))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install matplotlib  # Uncomment and run if matplotlib is not installed yet.\n",
    "import matplotlib          # We use this package to visualize some data and results\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.train_images(), mnist.train_labels()\n",
    "X_test,  y_test  = mnist.test_images(), mnist.test_labels()\n",
    "num_classes = 10    # classes are the digits from 0 to 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las imágenes tienen 28x28 pixeles cada una."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de una de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = np.random.randint(0, X_test.shape[0])\n",
    "plt.imshow(X_test[img_idx], cmap=matplotlib.cm.binary)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[img_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, nuestras imágenes coinciden con su etiqueta de verdad sobre el terreno, ¡lo cual es una buena noticia!\n",
    "\n",
    "Sin embargo, como nuestra red solo acepta vectores de columna, necesitamos _aplanar_ las imágenes en vectores 1D, es decir, vectores de forma `(1, 784)` (ya que $28 \\times 28 = 784$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(-1, 28 * 28), X_test.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train / 255., X_test / 255.\n",
    "print(\"Normalized pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.eye(num_classes)[y_train] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación del clasificador\n",
    "\n",
    "Es hora de preparar el propio clasificador. Usemos nuestra clase `SimpleNetwork` e instanciamos una red con 2 capas ocultas, tomando una imagen aplanada como entrada y devolviendo un vector de 10 valores que representa su creencia de que la imagen pertenece a cada una de las clases (cuanto mayor sea el valor, más fuerte será la creencia):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier = SimpleNetwork(num_inputs=X_train.shape[1], \n",
    "                                 num_outputs=num_classes, hidden_layers_sizes=[64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = mnist_classifier.forward(X_train)                         # forward pass\n",
    "loss_untrained = mnist_classifier.loss_function(predictions, y_train)   # loss computation\n",
    "\n",
    "accuracy_untrained = mnist_classifier.evaluate_accuracy(X_test, y_test)  # Accuracy\n",
    "print(\"Untrained : training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "    loss_untrained, accuracy_untrained * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = mnist_classifier.train(X_train, y_train, X_test, y_test, \n",
    "                                            batch_size=30, num_epochs=60) #500\n",
    "# note: Reduce the batch size and/or number of epochs if your computer can't \n",
    "#       handle the computations / takes too long.\n",
    "#       Remember, numpy also uses the CPU, not GPUs as modern Deep Learning \n",
    "#       libraries do, hence the lack of computational performance here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use `np.expand_dims(x, 0)` to simulate a batch (transforming the image shape\n",
    "# from (784,) to (1, 784)):\n",
    "predicted_class = mnist_classifier.predict(np.expand_dims(X_test[img_idx], 0))\n",
    "print('Predicted class: {}; Correct class: {}'.format(predicted_class, y_test[img_idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicio\n",
    "\n",
    "1. Cree una versión de la red y córrala sin entrenar con unas 8 imágenes elegidas aleatoriamente.\n",
    "2. Entrene la red hasta 80, 100 y 140 `epochs` y pruébela cada versión con las mismas 8 imágenes seleccionadas para el punto anterior.\n",
    "3. Cambie el número de `batch_size` (mayor o menor, según la capacidad de su computador) y repita el ejercicio.\n",
    "4. Escriba sus conclusiones sobre el efecto que tiene cambiar estos dos valores."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ids-ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f601792c76910078458e470cd9c93a0ebec751bec328425a1820cc8688c8ca05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
